Simple game starting...
Episode 0, Step 0Episode 0, Step 1Episode 0, Step 2Episode 0, Step 3Episode 0, Step 4Episode 0, Step 5Episode 0, Step 6Episode 0, Step 7Episode 0, Step 8Episode 0, Step 9

Episode 0 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 0 finished, Train info:
Value loss: 0.9999998807907104
Action loss: 1.5199184133507515e-08
distEntropy loss: -0.36162903904914856
Loss: 0.4963836669921875
total_episode_reward: -1.7881393432617188e-07
mean_episode_reward: -1.7881394143159923e-08
num_steps: 10
Episode 1, Step 0Episode 1, Step 1Episode 1, Step 2Episode 1, Step 3Episode 1, Step 4Episode 1, Step 5Episode 1, Step 6Episode 1, Step 7Episode 1, Step 8Episode 1, Step 9

Episode 1 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 1 finished, Train info:
Value loss: 1.0000001192092896
Action loss: 5.881302111987452e-09
distEntropy loss: -0.253194659948349
Loss: 0.49746811389923096
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 2, Step 0Episode 2, Step 1Episode 2, Step 2Episode 2, Step 3Episode 2, Step 4Episode 2, Step 5Episode 2, Step 6Episode 2, Step 7Episode 2, Step 8Episode 2, Step 9

Episode 2 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 2 finished, Train info:
Value loss: 0.9999997019767761
Action loss: -2.3003667859455845e-09
distEntropy loss: -0.20773382484912872
Loss: 0.49792250990867615
total_episode_reward: -1.4901161193847656e-07
mean_episode_reward: -1.4901161193847656e-08
num_steps: 10
Episode 3, Step 0Episode 3, Step 1Episode 3, Step 2Episode 3, Step 3Episode 3, Step 4Episode 3, Step 5Episode 3, Step 6Episode 3, Step 7Episode 3, Step 8Episode 3, Step 9

Episode 3 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 3 finished, Train info:
Value loss: 0.9999998807907104
Action loss: 4.470348358154297e-08
distEntropy loss: -0.6931262016296387
Loss: 0.49306872487068176
total_episode_reward: 1.7881393432617188e-07
mean_episode_reward: 1.7881394143159923e-08
num_steps: 10
Episode 4, Step 0Episode 4, Step 1Episode 4, Step 2Episode 4, Step 3Episode 4, Step 4Episode 4, Step 5Episode 4, Step 6Episode 4, Step 7Episode 4, Step 8Episode 4, Step 9

Episode 4 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 4 finished, Train info:
Value loss: 1.0
Action loss: 2.8088688708294285e-08
distEntropy loss: -0.3515790104866028
Loss: 0.4964842200279236
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 5, Step 0Episode 5, Step 1Episode 5, Step 2Episode 5, Step 3Episode 5, Step 4Episode 5, Step 5Episode 5, Step 6Episode 5, Step 7Episode 5, Step 8Episode 5, Step 9

Episode 5 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 5 finished, Train info:
Value loss: 1.0000001192092896
Action loss: -4.390254559893947e-08
distEntropy loss: -0.3619038462638855
Loss: 0.4963809549808502
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 6, Step 0Episode 6, Step 1Episode 6, Step 2Episode 6, Step 3Episode 6, Step 4Episode 6, Step 5Episode 6, Step 6Episode 6, Step 7Episode 6, Step 8Episode 6, Step 9

Episode 6 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 6 finished, Train info:
Value loss: 0.9999998807907104
Action loss: -1.3560057077199872e-08
distEntropy loss: -0.28285616636276245
Loss: 0.4971713721752167
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 7, Step 0Episode 7, Step 1Episode 7, Step 2Episode 7, Step 3Episode 7, Step 4Episode 7, Step 5Episode 7, Step 6Episode 7, Step 7Episode 7, Step 8Episode 7, Step 9

Episode 7 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 7 finished, Train info:
Value loss: 1.0
Action loss: -9.53674295089968e-09
distEntropy loss: -0.6930898427963257
Loss: 0.4930691123008728
total_episode_reward: -1.1920928955078125e-07
mean_episode_reward: -1.1920929132713809e-08
num_steps: 10
Episode 8, Step 0Episode 8, Step 1Episode 8, Step 2Episode 8, Step 3Episode 8, Step 4Episode 8, Step 5Episode 8, Step 6Episode 8, Step 7Episode 8, Step 8Episode 8, Step 9

Episode 8 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 8 finished, Train info:
Value loss: 1.0
Action loss: 1.19209286886246e-09
distEntropy loss: -0.27544164657592773
Loss: 0.49724557995796204
total_episode_reward: 1.7881393432617188e-07
mean_episode_reward: 1.7881394143159923e-08
num_steps: 10
Episode 9, Step 0Episode 9, Step 1Episode 9, Step 2Episode 9, Step 3Episode 9, Step 4Episode 9, Step 5Episode 9, Step 6Episode 9, Step 7Episode 9, Step 8Episode 9, Step 9

Episode 9 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 9 finished, Train info:
Value loss: 1.0
Action loss: -1.6018748993928966e-08
distEntropy loss: -0.2621102035045624
Loss: 0.4973788559436798
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 10, Step 0Episode 10, Step 1Episode 10, Step 2Episode 10, Step 3Episode 10, Step 4Episode 10, Step 5Episode 10, Step 6Episode 10, Step 7Episode 10, Step 8Episode 10, Step 9

Episode 10 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 10 finished, Train info:
Value loss: 1.0
Action loss: 3.233552092751779e-08
distEntropy loss: -0.2890069782733917
Loss: 0.497109979391098
total_episode_reward: -1.1920928955078125e-07
mean_episode_reward: -1.1920929132713809e-08
num_steps: 10
Episode 11, Step 0Episode 11, Step 1Episode 11, Step 2Episode 11, Step 3Episode 11, Step 4Episode 11, Step 5Episode 11, Step 6Episode 11, Step 7Episode 11, Step 8Episode 11, Step 9

Episode 11 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Warning: Gradient norm overflow: 5.0090042810995016e+35, skipping update.
Episode 11 finished, Train info:
Value loss: 1.0
Action loss: -4.880130077822287e-09
distEntropy loss: -0.23277027904987335
Loss: 0.49767228960990906
total_episode_reward: 2.384185791015625e-07
mean_episode_reward: 2.3841858265427618e-08
num_steps: 10
Episode 12, Step 0Episode 12, Step 1Episode 12, Step 2Episode 12, Step 3Episode 12, Step 4Episode 12, Step 5Episode 12, Step 6Episode 12, Step 7Episode 12, Step 8Episode 12, Step 9

Episode 12 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 12 finished, Train info:
Value loss: 1.0
Action loss: -1.7434357957313296e-08
distEntropy loss: -0.30150288343429565
Loss: 0.49698492884635925
total_episode_reward: 1.7881393432617188e-07
mean_episode_reward: 1.7881394143159923e-08
num_steps: 10
Episode 13, Step 0Episode 13, Step 1Episode 13, Step 2Episode 13, Step 3Episode 13, Step 4Episode 13, Step 5Episode 13, Step 6Episode 13, Step 7Episode 13, Step 8Episode 13, Step 9

Episode 13 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 13 finished, Train info:
Value loss: 1.0
Action loss: -2.2947787670091202e-08
distEntropy loss: -0.6931092143058777
Loss: 0.4930688738822937
total_episode_reward: 2.384185791015625e-07
mean_episode_reward: 2.3841858265427618e-08
num_steps: 10
Episode 14, Step 0Episode 14, Step 1Episode 14, Step 2Episode 14, Step 3Episode 14, Step 4Episode 14, Step 5Episode 14, Step 6Episode 14, Step 7Episode 14, Step 8Episode 14, Step 9

Episode 14 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 14 finished, Train info:
Value loss: 1.0000001192092896
Action loss: 6.342306591022862e-08
distEntropy loss: -0.39644768834114075
Loss: 0.496035635471344
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 15, Step 0Episode 15, Step 1Episode 15, Step 2Episode 15, Step 3Episode 15, Step 4Episode 15, Step 5Episode 15, Step 6Episode 15, Step 7Episode 15, Step 8Episode 15, Step 9

Episode 15 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 15 finished, Train info:
Value loss: 1.0
Action loss: -1.0468065525515158e-08
distEntropy loss: -0.2534925043582916
Loss: 0.4974650740623474
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 16, Step 0Episode 16, Step 1Episode 16, Step 2Episode 16, Step 3Episode 16, Step 4Episode 16, Step 5Episode 16, Step 6Episode 16, Step 7Episode 16, Step 8Episode 16, Step 9

Episode 16 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 16 finished, Train info:
Value loss: 0.9999998807907104
Action loss: 1.974403929239088e-08
distEntropy loss: -0.2703053951263428
Loss: 0.49729692935943604
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 17, Step 0Episode 17, Step 1Episode 17, Step 2Episode 17, Step 3Episode 17, Step 4Episode 17, Step 5Episode 17, Step 6Episode 17, Step 7Episode 17, Step 8Episode 17, Step 9

Episode 17 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 17 finished, Train info:
Value loss: 1.0
Action loss: 2.210959770820864e-08
distEntropy loss: -0.2695878744125366
Loss: 0.49730411171913147
total_episode_reward: 1.1920928955078125e-07
mean_episode_reward: 1.1920929132713809e-08
num_steps: 10
Episode 18, Step 0Episode 18, Step 1Episode 18, Step 2Episode 18, Step 3Episode 18, Step 4Episode 18, Step 5Episode 18, Step 6Episode 18, Step 7Episode 18, Step 8Episode 18, Step 9

Episode 18 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 18 finished, Train info:
Value loss: 1.0
Action loss: 2.309679913992113e-08
distEntropy loss: -0.32882195711135864
Loss: 0.496711790561676
total_episode_reward: -2.682209014892578e-07
mean_episode_reward: -2.6822089438383045e-08
num_steps: 10
Episode 19, Step 0Episode 19, Step 1Episode 19, Step 2Episode 19, Step 3Episode 19, Step 4Episode 19, Step 5Episode 19, Step 6Episode 19, Step 7Episode 19, Step 8Episode 19, Step 9

Episode 19 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 19 finished, Train info:
Value loss: 0.9999998211860657
Action loss: -2.9206276508375595e-08
distEntropy loss: -0.23575952649116516
Loss: 0.49764227867126465
total_episode_reward: -1.4901161193847656e-07
mean_episode_reward: -1.4901161193847656e-08
num_steps: 10
Episode 20, Step 0Episode 20, Step 1Episode 20, Step 2Episode 20, Step 3Episode 20, Step 4Episode 20, Step 5Episode 20, Step 6Episode 20, Step 7Episode 20, Step 8Episode 20, Step 9

Episode 20 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 20 finished, Train info:
Value loss: 1.0000001192092896
Action loss: 4.172325152040912e-09
distEntropy loss: -0.6931358575820923
Loss: 0.4930686950683594
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 21, Step 0Episode 21, Step 1Episode 21, Step 2Episode 21, Step 3Episode 21, Step 4Episode 21, Step 5Episode 21, Step 6Episode 21, Step 7Episode 21, Step 8Episode 21, Step 9

Episode 21 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 21 finished, Train info:
Value loss: 1.0
Action loss: 2.38418573772492e-09
distEntropy loss: -0.3471451997756958
Loss: 0.4965285360813141
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 22, Step 0Episode 22, Step 1Episode 22, Step 2Episode 22, Step 3Episode 22, Step 4Episode 22, Step 5Episode 22, Step 6Episode 22, Step 7Episode 22, Step 8Episode 22, Step 9

Episode 22 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 22 finished, Train info:
Value loss: 1.0000001192092896
Action loss: -6.593763757223314e-09
distEntropy loss: -0.3001972734928131
Loss: 0.4969981014728546
total_episode_reward: -1.7881393432617188e-07
mean_episode_reward: -1.7881394143159923e-08
num_steps: 10
Episode 23, Step 0Episode 23, Step 1Episode 23, Step 2Episode 23, Step 3Episode 23, Step 4Episode 23, Step 5Episode 23, Step 6Episode 23, Step 7Episode 23, Step 8Episode 23, Step 9

Episode 23 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 23 finished, Train info:
Value loss: 1.0
Action loss: 3.5008415721904385e-08
distEntropy loss: -0.42426204681396484
Loss: 0.49575743079185486
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 24, Step 0Episode 24, Step 1Episode 24, Step 2Episode 24, Step 3Episode 24, Step 4Episode 24, Step 5Episode 24, Step 6Episode 24, Step 7Episode 24, Step 8Episode 24, Step 9

Episode 24 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 24 finished, Train info:
Value loss: 1.0
Action loss: 1.2069940602543738e-08
distEntropy loss: -0.40304526686668396
Loss: 0.4959695339202881
total_episode_reward: -4.76837158203125e-07
mean_episode_reward: -4.7683716530855236e-08
num_steps: 10
Episode 25, Step 0Episode 25, Step 1Episode 25, Step 2Episode 25, Step 3Episode 25, Step 4Episode 25, Step 5Episode 25, Step 6Episode 25, Step 7Episode 25, Step 8Episode 25, Step 9

Episode 25 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 25 finished, Train info:
Value loss: 1.0000001192092896
Action loss: 2.38418573772492e-09
distEntropy loss: -0.4704081118106842
Loss: 0.4952959716320038
total_episode_reward: 2.384185791015625e-07
mean_episode_reward: 2.3841858265427618e-08
num_steps: 10
Episode 26, Step 0Episode 26, Step 1Episode 26, Step 2Episode 26, Step 3Episode 26, Step 4Episode 26, Step 5Episode 26, Step 6Episode 26, Step 7Episode 26, Step 8Episode 26, Step 9

Episode 26 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 26 finished, Train info:
Value loss: 1.0
Action loss: -9.53674295089968e-09
distEntropy loss: -0.3631792962551117
Loss: 0.4963681995868683
total_episode_reward: -2.980232238769531e-07
mean_episode_reward: -2.9802322387695312e-08
num_steps: 10
Episode 27, Step 0Episode 27, Step 1Episode 27, Step 2Episode 27, Step 3Episode 27, Step 4Episode 27, Step 5Episode 27, Step 6Episode 27, Step 7Episode 27, Step 8Episode 27, Step 9

Episode 27 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 27 finished, Train info:
Value loss: 1.0
Action loss: 1.3709068547029801e-08
distEntropy loss: -0.37240299582481384
Loss: 0.49627596139907837
total_episode_reward: 1.1920928955078125e-07
mean_episode_reward: 1.1920929132713809e-08
num_steps: 10
Episode 28, Step 0Episode 28, Step 1Episode 28, Step 2Episode 28, Step 3Episode 28, Step 4Episode 28, Step 5Episode 28, Step 6Episode 28, Step 7Episode 28, Step 8Episode 28, Step 9

Episode 28 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 28 finished, Train info:
Value loss: 1.0
Action loss: 1.19209286886246e-09
distEntropy loss: -0.47064104676246643
Loss: 0.49529358744621277
total_episode_reward: 1.1920928955078125e-07
mean_episode_reward: 1.1920929132713809e-08
num_steps: 10
Episode 29, Step 0Episode 29, Step 1Episode 29, Step 2Episode 29, Step 3Episode 29, Step 4Episode 29, Step 5Episode 29, Step 6Episode 29, Step 7Episode 29, Step 8Episode 29, Step 9

Episode 29 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 29 finished, Train info:
Value loss: 1.0
Action loss: -4.187226210206063e-08
distEntropy loss: -0.39161139726638794
Loss: 0.4960838556289673
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 30, Step 0Episode 30, Step 1Episode 30, Step 2Episode 30, Step 3Episode 30, Step 4Episode 30, Step 5Episode 30, Step 6Episode 30, Step 7Episode 30, Step 8Episode 30, Step 9

Episode 30 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 30 finished, Train info:
Value loss: 1.000000238418579
Action loss: 1.4305114426349519e-08
distEntropy loss: -0.21123242378234863
Loss: 0.4978877902030945
total_episode_reward: -1.1920928955078125e-07
mean_episode_reward: -1.1920929132713809e-08
num_steps: 10
Episode 31, Step 0Episode 31, Step 1Episode 31, Step 2Episode 31, Step 3Episode 31, Step 4Episode 31, Step 5Episode 31, Step 6Episode 31, Step 7Episode 31, Step 8Episode 31, Step 9

Episode 31 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Warning: Gradient norm overflow: 1.4307825548965734e+21, skipping update.
Episode 31 finished, Train info:
Value loss: 1.000000238418579
Action loss: -9.53674295089968e-09
distEntropy loss: -0.6923969388008118
Loss: 0.4930761456489563
total_episode_reward: 1.7881393432617188e-07
mean_episode_reward: 1.7881394143159923e-08
num_steps: 10
Episode 32, Step 0Episode 32, Step 1Episode 32, Step 2Episode 32, Step 3Episode 32, Step 4Episode 32, Step 5Episode 32, Step 6Episode 32, Step 7Episode 32, Step 8Episode 32, Step 9

Episode 32 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 32 finished, Train info:
Value loss: 1.0
Action loss: 6.8359078397861595e-09
distEntropy loss: -0.2908383011817932
Loss: 0.4970916211605072
total_episode_reward: 3.5762786865234375e-07
mean_episode_reward: 3.5762788286319847e-08
num_steps: 10
Episode 33, Step 0Episode 33, Step 1Episode 33, Step 2Episode 33, Step 3Episode 33, Step 4Episode 33, Step 5Episode 33, Step 6Episode 33, Step 7Episode 33, Step 8Episode 33, Step 9

Episode 33 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 33 finished, Train info:
Value loss: 1.0000001192092896
Action loss: -7.152557657263969e-08
distEntropy loss: -0.3873831629753113
Loss: 0.4961261749267578
total_episode_reward: 3.5762786865234375e-07
mean_episode_reward: 3.5762788286319847e-08
num_steps: 10
Episode 34, Step 0Episode 34, Step 1Episode 34, Step 2Episode 34, Step 3Episode 34, Step 4Episode 34, Step 5Episode 34, Step 6Episode 34, Step 7Episode 34, Step 8Episode 34, Step 9

Episode 34 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 34 finished, Train info:
Value loss: 0.9999997019767761
Action loss: 2.1383165460520104e-08
distEntropy loss: -0.3524499833583832
Loss: 0.49647536873817444
total_episode_reward: 1.1920928955078125e-07
mean_episode_reward: 1.1920929132713809e-08
num_steps: 10
Episode 35, Step 0Episode 35, Step 1Episode 35, Step 2Episode 35, Step 3Episode 35, Step 4Episode 35, Step 5Episode 35, Step 6Episode 35, Step 7Episode 35, Step 8Episode 35, Step 9

Episode 35 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 35 finished, Train info:
Value loss: 0.9999998807907104
Action loss: -5.215406329028838e-09
distEntropy loss: -0.6930985450744629
Loss: 0.49306896328926086
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 36, Step 0Episode 36, Step 1Episode 36, Step 2Episode 36, Step 3Episode 36, Step 4Episode 36, Step 5Episode 36, Step 6Episode 36, Step 7Episode 36, Step 8Episode 36, Step 9

Episode 36 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 36 finished, Train info:
Value loss: 0.9999998807907104
Action loss: 2.265442056170741e-09
distEntropy loss: -0.23951010406017303
Loss: 0.4976048469543457
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 37, Step 0Episode 37, Step 1Episode 37, Step 2Episode 37, Step 3Episode 37, Step 4Episode 37, Step 5Episode 37, Step 6Episode 37, Step 7Episode 37, Step 8Episode 37, Step 9

Episode 37 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 37 finished, Train info:
Value loss: 0.9999998211860657
Action loss: -1.028180136586343e-07
distEntropy loss: -0.2919498085975647
Loss: 0.49708032608032227
total_episode_reward: -2.682209014892578e-07
mean_episode_reward: -2.6822089438383045e-08
num_steps: 10
Episode 38, Step 0Episode 38, Step 1Episode 38, Step 2Episode 38, Step 3Episode 38, Step 4Episode 38, Step 5Episode 38, Step 6Episode 38, Step 7Episode 38, Step 8Episode 38, Step 9

Episode 38 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 38 finished, Train info:
Value loss: 0.9999998211860657
Action loss: -4.0009616952829674e-08
distEntropy loss: -0.29373425245285034
Loss: 0.4970625340938568
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 39, Step 0Episode 39, Step 1Episode 39, Step 2Episode 39, Step 3Episode 39, Step 4Episode 39, Step 5Episode 39, Step 6Episode 39, Step 7Episode 39, Step 8Episode 39, Step 9

Episode 39 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 39 finished, Train info:
Value loss: 1.0000001192092896
Action loss: -9.53674295089968e-09
distEntropy loss: -0.2414274513721466
Loss: 0.4975857734680176
total_episode_reward: -1.1920928955078125e-07
mean_episode_reward: -1.1920929132713809e-08
num_steps: 10
Episode 40, Step 0Episode 40, Step 1Episode 40, Step 2Episode 40, Step 3Episode 40, Step 4Episode 40, Step 5Episode 40, Step 6Episode 40, Step 7Episode 40, Step 8Episode 40, Step 9

Episode 40 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 40 finished, Train info:
Value loss: 0.9999998807907104
Action loss: 7.95349475168905e-09
distEntropy loss: -0.24124498665332794
Loss: 0.49758750200271606
total_episode_reward: 2.384185791015625e-07
mean_episode_reward: 2.3841858265427618e-08
num_steps: 10
Episode 41, Step 0Episode 41, Step 1Episode 41, Step 2Episode 41, Step 3Episode 41, Step 4Episode 41, Step 5Episode 41, Step 6Episode 41, Step 7Episode 41, Step 8Episode 41, Step 9

Episode 41 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 41 finished, Train info:
Value loss: 0.9999998807907104
Action loss: 8.940696516468449e-10
distEntropy loss: -0.3186585009098053
Loss: 0.4968133568763733
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 42, Step 0Episode 42, Step 1Episode 42, Step 2Episode 42, Step 3Episode 42, Step 4Episode 42, Step 5Episode 42, Step 6Episode 42, Step 7Episode 42, Step 8Episode 42, Step 9

Episode 42 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 42 finished, Train info:
Value loss: 1.0
Action loss: -2.9802322387695312e-08
distEntropy loss: -0.6931372880935669
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
SharLoss: 0.4930686056613922
total_episode_reward: 1.1920928955078125e-07
mean_episode_reward: 1.1920929132713809e-08
num_steps: 10
Episode 43, Step 0Episode 43, Step 1Episode 43, Step 2Episode 43, Step 3Episode 43, Step 4Episode 43, Step 5Episode 43, Step 6Episode 43, Step 7Episode 43, Step 8Episode 43, Step 9

Episode 43 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 43 finished, Train info:
Value loss: 1.0
Action loss: 1.0263175198588215e-08
distEntropy loss: -0.2966022193431854
Loss: 0.4970339834690094
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 44, Step 0Episode 44, Step 1Episode 44, Step 2Episode 44, Step 3Episode 44, Step 4Episode 44, Step 5Episode 44, Step 6Episode 44, Step 7Episode 44, Step 8Episode 44, Step 9

Episode 44 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 44 finished, Train info:
Value loss: 1.0000001192092896
Action loss: 4.470348358154297e-08
distEntropy loss: -0.6920846104621887
Loss: 0.493079274892807
total_episode_reward: 1.1920928955078125e-07
mean_episode_reward: 1.1920929132713809e-08
num_steps: 10
Episode 45, Step 0Episode 45, Step 1Episode 45, Step 2Episode 45, Step 3Episode 45, Step 4Episode 45, Step 5Episode 45, Step 6Episode 45, Step 7Episode 45, Step 8Episode 45, Step 9

Episode 45 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 45 finished, Train info:
Value loss: 0.9999998211860657
Action loss: -3.628432665436776e-08
distEntropy loss: -0.3255075216293335
Loss: 0.4967448115348816
total_episode_reward: -1.4901161193847656e-07
mean_episode_reward: -1.4901161193847656e-08
num_steps: 10
Episode 46, Step 0Episode 46, Step 1Episode 46, Step 2Episode 46, Step 3Episode 46, Step 4Episode 46, Step 5Episode 46, Step 6Episode 46, Step 7Episode 46, Step 8Episode 46, Step 9

Episode 46 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 46 finished, Train info:
Value loss: 1.0
Action loss: 2.086162576020456e-09
distEntropy loss: -0.37932252883911133
Loss: 0.49620676040649414
total_episode_reward: 2.384185791015625e-07
mean_episode_reward: 2.3841858265427618e-08
num_steps: 10
Episode 47, Step 0Episode 47, Step 1Episode 47, Step 2Episode 47, Step 3Episode 47, Step 4Episode 47, Step 5Episode 47, Step 6Episode 47, Step 7Episode 47, Step 8Episode 47, Step 9

Episode 47 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 47 finished, Train info:
Value loss: 1.0
Action loss: -1.1920929132713809e-08
distEntropy loss: -0.6931134462356567
Loss: 0.4930688738822937
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 48, Step 0Episode 48, Step 1Episode 48, Step 2Episode 48, Step 3Episode 48, Step 4Episode 48, Step 5Episode 48, Step 6Episode 48, Step 7Episode 48, Step 8Episode 48, Step 9

Episode 48 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 48 finished, Train info:
Value loss: 1.0
Action loss: -8.419156038996789e-09
distEntropy loss: -0.31055766344070435
Loss: 0.4968944191932678
total_episode_reward: -1.4901161193847656e-07
mean_episode_reward: -1.4901161193847656e-08
num_steps: 10
Episode 49, Step 0Episode 49, Step 1Episode 49, Step 2Episode 49, Step 3Episode 49, Step 4Episode 49, Step 5Episode 49, Step 6Episode 49, Step 7Episode 49, Step 8Episode 49, Step 9

Episode 49 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 49 finished, Train info:
Value loss: 0.9999998211860657
Action loss: -1.5735626845980732e-07
distEntropy loss: -0.6559782028198242
Loss: 0.4934399724006653
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 50, Step 0Episode 50, Step 1Episode 50, Step 2Episode 50, Step 3Episode 50, Step 4Episode 50, Step 5Episode 50, Step 6Episode 50, Step 7Episode 50, Step 8Episode 50, Step 9

Episode 50 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 50 finished, Train info:
Value loss: 0.9999998807907104
Action loss: -4.172325152040912e-09
distEntropy loss: -0.6931309700012207
Loss: 0.4930686354637146
total_episode_reward: 1.7881393432617188e-07
mean_episode_reward: 1.7881394143159923e-08
num_steps: 10
Episode 51, Step 0Episode 51, Step 1Episode 51, Step 2Episode 51, Step 3Episode 51, Step 4Episode 51, Step 5Episode 51, Step 6Episode 51, Step 7Episode 51, Step 8Episode 51, Step 9

Episode 51 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 51 finished, Train info:
Value loss: 1.0000001192092896
Action loss: 2.5450136220683817e-08
distEntropy loss: -0.31122446060180664
Loss: 0.49688780307769775
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 52, Step 0Episode 52, Step 1Episode 52, Step 2Episode 52, Step 3Episode 52, Step 4Episode 52, Step 5Episode 52, Step 6Episode 52, Step 7Episode 52, Step 8Episode 52, Step 9

Episode 52 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 52 finished, Train info:
Value loss: 0.9999998807907104
Action loss: -6.158835930136775e-08
distEntropy loss: -0.29333406686782837
Loss: 0.49706652760505676
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 53, Step 0Episode 53, Step 1Episode 53, Step 2Episode 53, Step 3Episode 53, Step 4Episode 53, Step 5Episode 53, Step 6Episode 53, Step 7Episode 53, Step 8Episode 53, Step 9

Episode 53 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 53 finished, Train info:
Value loss: 1.0000001192092896
Action loss: 4.712492174263616e-09
distEntropy loss: -0.23236992955207825
Loss: 0.49767637252807617
total_episode_reward: 2.384185791015625e-07
mean_episode_reward: 2.3841858265427618e-08
num_steps: 10
Episode 54, Step 0Episode 54, Step 1Episode 54, Step 2Episode 54, Step 3Episode 54, Step 4Episode 54, Step 5Episode 54, Step 6Episode 54, Step 7Episode 54, Step 8Episode 54, Step 9

Episode 54 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 54 finished, Train info:
Value loss: 0.9999998807907104
Action loss: 1.518521486332247e-08
distEntropy loss: -0.2765863835811615
Loss: 0.4972341060638428
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 55, Step 0Episode 55, Step 1Episode 55, Step 2Episode 55, Step 3Episode 55, Step 4Episode 55, Step 5Episode 55, Step 6Episode 55, Step 7Episode 55, Step 8Episode 55, Step 9

Episode 55 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 55 finished, Train info:
Value loss: 1.0000001192092896
Action loss: -4.0046872484822416e-09
distEntropy loss: -0.29298898577690125
Loss: 0.49707016348838806
total_episode_reward: -1.4901161193847656e-07
mean_episode_reward: -1.4901161193847656e-08
num_steps: 10
Episode 56, Step 0Episode 56, Step 1Episode 56, Step 2Episode 56, Step 3Episode 56, Step 4Episode 56, Step 5Episode 56, Step 6Episode 56, Step 7Episode 56, Step 8Episode 56, Step 9

Episode 56 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 56 finished, Train info:
Value loss: 0.9999998807907104
Action loss: 3.695488004495928e-08
distEntropy loss: -0.6541718244552612
Loss: 0.49345824122428894
total_episode_reward: 1.1920928955078125e-07
mean_episode_reward: 1.1920929132713809e-08
num_steps: 10
Episode 57, Step 0Episode 57, Step 1Episode 57, Step 2Episode 57, Step 3Episode 57, Step 4Episode 57, Step 5Episode 57, Step 6Episode 57, Step 7Episode 57, Step 8Episode 57, Step 9

Episode 57 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 57 finished, Train info:
Value loss: 0.9999998211860657
Action loss: 2.4288892674917406e-08
distEntropy loss: -0.2913622260093689
Loss: 0.4970863163471222
total_episode_reward: -1.1920928955078125e-07
mean_episode_reward: -1.1920929132713809e-08
num_steps: 10
Episode 58, Step 0Episode 58, Step 1Episode 58, Step 2Episode 58, Step 3Episode 58, Step 4Episode 58, Step 5Episode 58, Step 6Episode 58, Step 7Episode 58, Step 8Episode 58, Step 9

Episode 58 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 58 finished, Train info:
Value loss: 0.9999998807907104
Action loss: -1.4901161193847656e-08
distEntropy loss: -0.3057078719139099
Loss: 0.4969428479671478
total_episode_reward: -1.4901161193847656e-07
mean_episode_reward: -1.4901161193847656e-08
num_steps: 10
Episode 59, Step 0Episode 59, Step 1Episode 59, Step 2Episode 59, Step 3Episode 59, Step 4Episode 59, Step 5Episode 59, Step 6Episode 59, Step 7Episode 59, Step 8Episode 59, Step 9

Episode 59 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 59 finished, Train info:
Value loss: 1.0000001192092896
Action loss: -2.2351741790771484e-08
distEntropy loss: -0.4256937503814697
Loss: 0.49574312567710876
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 60, Step 0Episode 60, Step 1Episode 60, Step 2Episode 60, Step 3Episode 60, Step 4Episode 60, Step 5Episode 60, Step 6Episode 60, Step 7Episode 60, Step 8Episode 60, Step 9

Episode 60 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 60 finished, Train info:
Value loss: 1.0
Action loss: -5.7667492114887864e-08
distEntropy loss: -0.2243904173374176
Loss: 0.4977560341358185
total_episode_reward: 2.384185791015625e-07
mean_episode_reward: 2.3841858265427618e-08
num_steps: 10
Episode 61, Step 0Episode 61, Step 1Episode 61, Step 2Episode 61, Step 3Episode 61, Step 4Episode 61, Step 5Episode 61, Step 6Episode 61, Step 7Episode 61, Step 8Episode 61, Step 9

Episode 61 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 61 finished, Train info:
Value loss: 1.0
Action loss: -2.1187588572502136e-08
distEntropy loss: -0.23560187220573425
Loss: 0.49764394760131836
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 62, Step 0Episode 62, Step 1Episode 62, Step 2Episode 62, Step 3Episode 62, Step 4Episode 62, Step 5Episode 62, Step 6Episode 62, Step 7Episode 62, Step 8Episode 62, Step 9

Episode 62 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 62 finished, Train info:
Value loss: 1.000000238418579
Action loss: 4.470348535789981e-09
distEntropy loss: -0.6931262016296387
Loss: 0.4930688440799713
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 63, Step 0Episode 63, Step 1Episode 63, Step 2Episode 63, Step 3Episode 63, Step 4Episode 63, Step 5Episode 63, Step 6Episode 63, Step 7Episode 63, Step 8Episode 63, Step 9

Episode 63 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 63 finished, Train info:
Value loss: 1.0
Action loss: -4.61936000561991e-09
distEntropy loss: -0.30063802003860474
Loss: 0.49699363112449646
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 64, Step 0Episode 64, Step 1Episode 64, Step 2Episode 64, Step 3Episode 64, Step 4Episode 64, Step 5Episode 64, Step 6Episode 64, Step 7Episode 64, Step 8Episode 64, Step 9

Episode 64 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 64 finished, Train info:
Value loss: 1.0
Action loss: 7.543712876589836e-10
distEntropy loss: -0.34130826592445374
Loss: 0.4965869188308716
total_episode_reward: 1.1920928955078125e-07
mean_episode_reward: 1.1920929132713809e-08
num_steps: 10
Episode 65, Step 0Episode 65, Step 1Episode 65, Step 2Episode 65, Step 3Episode 65, Step 4Episode 65, Step 5Episode 65, Step 6Episode 65, Step 7Episode 65, Step 8Episode 65, Step 9

Episode 65 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 65 finished, Train info:
Value loss: 1.0
Action loss: 5.7220457705398076e-08
distEntropy loss: -0.1692887395620346
Loss: 0.49830716848373413
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 66, Step 0Episode 66, Step 1Episode 66, Step 2Episode 66, Step 3Episode 66, Step 4Episode 66, Step 5Episode 66, Step 6Episode 66, Step 7Episode 66, Step 8Episode 66, Step 9

Episode 66 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 66 finished, Train info:
Value loss: 0.9999998807907104
Action loss: 2.38418573772492e-09
distEntropy loss: -0.35947734117507935
Loss: 0.49640515446662903
total_episode_reward: 1.7881393432617188e-07
mean_episode_reward: 1.7881394143159923e-08
num_steps: 10
Episode 67, Step 0Episode 67, Step 1Episode 67, Step 2Episode 67, Step 3Episode 67, Step 4Episode 67, Step 5Episode 67, Step 6Episode 67, Step 7Episode 67, Step 8Episode 67, Step 9

Episode 67 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 67 finished, Train info:
Value loss: 1.0
Action loss: 5.395849989042745e-09
distEntropy loss: -0.2062191218137741
Loss: 0.49793779850006104
total_episode_reward: -1.4901161193847656e-07
mean_episode_reward: -1.4901161193847656e-08
num_steps: 10
Episode 68, Step 0Episode 68, Step 1Episode 68, Step 2Episode 68, Step 3Episode 68, Step 4Episode 68, Step 5Episode 68, Step 6Episode 68, Step 7Episode 68, Step 8Episode 68, Step 9

Episode 68 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 68 finished, Train info:
Value loss: 1.0
Action loss: -1.0728836485895954e-08
distEntropy loss: -0.6931419968605042
Loss: 0.4930685758590698
total_episode_reward: -1.4901161193847656e-07
mean_episode_reward: -1.4901161193847656e-08
num_steps: 10
Episode 69, Step 0Episode 69, Step 1Episode 69, Step 2Episode 69, Step 3Episode 69, Step 4Episode 69, Step 5Episode 69, Step 6Episode 69, Step 7Episode 69, Step 8Episode 69, Step 9

Episode 69 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 69 finished, Train info:
Value loss: 1.0000001192092896
Action loss: -1.19209286886246e-09
distEntropy loss: -0.6931461095809937
Loss: 0.4930686056613922
total_episode_reward: -2.682209014892578e-07
mean_episode_reward: -2.6822089438383045e-08
num_steps: 10
Episode 70, Step 0Episode 70, Step 1Episode 70, Step 2Episode 70, Step 3Episode 70, Step 4Episode 70, Step 5Episode 70, Step 6Episode 70, Step 7Episode 70, Step 8Episode 70, Step 9

Episode 70 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 70 finished, Train info:
Value loss: 1.0
Action loss: 8.344650304081824e-09
distEntropy loss: -0.6931363940238953
Loss: 0.4930686354637146
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 71, Step 0Episode 71, Step 1Episode 71, Step 2Episode 71, Step 3Episode 71, Step 4Episode 71, Step 5Episode 71, Step 6Episode 71, Step 7Episode 71, Step 8Episode 71, Step 9

Episode 71 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 71 finished, Train info:
Value loss: 1.0
Action loss: 2.771616003371946e-08
distEntropy loss: -0.4243709444999695
Loss: 0.4957562983036041
total_episode_reward: 2.384185791015625e-07
mean_episode_reward: 2.3841858265427618e-08
num_steps: 10
Episode 72, Step 0Episode 72, Step 1Episode 72, Step 2Episode 72, Step 3Episode 72, Step 4Episode 72, Step 5Episode 72, Step 6Episode 72, Step 7Episode 72, Step 8Episode 72, Step 9

Episode 72 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 72 finished, Train info:
Value loss: 1.0000001192092896
Action loss: 4.390254559893947e-08
distEntropy loss: -0.32554662227630615
Loss: 0.49674466252326965
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 73, Step 0Episode 73, Step 1Episode 73, Step 2Episode 73, Step 3Episode 73, Step 4Episode 73, Step 5Episode 73, Step 6Episode 73, Step 7Episode 73, Step 8Episode 73, Step 9

Episode 73 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 73 finished, Train info:
Value loss: 0.9999998807907104
Action loss: 4.76837147544984e-09
distEntropy loss: -0.3186863362789154
Loss: 0.4968130886554718
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 74, Step 0Episode 74, Step 1Episode 74, Step 2Episode 74, Step 3Episode 74, Step 4Episode 74, Step 5Episode 74, Step 6Episode 74, Step 7Episode 74, Step 8Episode 74, Step 9

Episode 74 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 74 finished, Train info:
Value loss: 1.0
Action loss: 1.7881394143159923e-08
distEntropy loss: -0.6931278109550476
Loss: 0.49306872487068176
total_episode_reward: 1.1920928955078125e-07
mean_episode_reward: 1.1920929132713809e-08
num_steps: 10
Episode 75, Step 0Episode 75, Step 1Episode 75, Step 2Episode 75, Step 3Episode 75, Step 4Episode 75, Step 5Episode 75, Step 6Episode 75, Step 7Episode 75, Step 8Episode 75, Step 9

Episode 75 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 75 finished, Train info:
Value loss: 0.9999998211860657
Action loss: 2.9802322831784522e-09
distEntropy loss: -0.2915986180305481
Loss: 0.4970839321613312
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 76, Step 0Episode 76, Step 1Episode 76, Step 2Episode 76, Step 3Episode 76, Step 4Episode 76, Step 5Episode 76, Step 6Episode 76, Step 7Episode 76, Step 8Episode 76, Step 9

Episode 76 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 76 finished, Train info:
Value loss: 1.0000001192092896
Action loss: -3.814697180359872e-08
distEntropy loss: -0.6931406259536743
Loss: 0.4930686056613922
total_episode_reward: 1.1920928955078125e-07
mean_episode_reward: 1.1920929132713809e-08
num_steps: 10
Episode 77, Step 0Episode 77, Step 1Episode 77, Step 2Episode 77, Step 3Episode 77, Step 4Episode 77, Step 5Episode 77, Step 6Episode 77, Step 7Episode 77, Step 8Episode 77, Step 9

Episode 77 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 77 finished, Train info:
Value loss: 1.0000001192092896
Action loss: 1.609325472884393e-08
distEntropy loss: -0.6931266188621521
Loss: 0.49306878447532654
total_episode_reward: -2.682209014892578e-07
mean_episode_reward: -2.6822089438383045e-08
num_steps: 10
Episode 78, Step 0Episode 78, Step 1Episode 78, Step 2Episode 78, Step 3Episode 78, Step 4Episode 78, Step 5Episode 78, Step 6Episode 78, Step 7Episode 78, Step 8Episode 78, Step 9

Episode 78 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 78 finished, Train info:
Value loss: 1.0
Action loss: -2.957880518295042e-08
distEntropy loss: -0.2924194037914276
Loss: 0.49707576632499695
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 79, Step 0Episode 79, Step 1Episode 79, Step 2Episode 79, Step 3Episode 79, Step 4Episode 79, Step 5Episode 79, Step 6Episode 79, Step 7Episode 79, Step 8Episode 79, Step 9

Episode 79 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 79 finished, Train info:
Value loss: 1.0
Action loss: 4.968605971100715e-09
distEntropy loss: -0.2470400631427765
Loss: 0.4975295960903168
total_episode_reward: -2.384185791015625e-07
mean_episode_reward: -2.3841858265427618e-08
num_steps: 10
Episode 80, Step 0Episode 80, Step 1Episode 80, Step 2Episode 80, Step 3Episode 80, Step 4Episode 80, Step 5Episode 80, Step 6Episode 80, Step 7Episode 80, Step 8Episode 80, Step 9

Episode 80 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 80 finished, Train info:
Value loss: 1.0000001192092896
Action loss: -8.344650304081824e-09
distEntropy loss: -0.693131685256958
Loss: 0.49306875467300415
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 81, Step 0Episode 81, Step 1Episode 81, Step 2Episode 81, Step 3Episode 81, Step 4Episode 81, Step 5Episode 81, Step 6Episode 81, Step 7Episode 81, Step 8Episode 81, Step 9

Episode 81 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 81 finished, Train info:
Value loss: 1.0000001192092896
Action loss: -1.7881393032936899e-09
distEntropy loss: -0.37650755047798157
Loss: 0.4962349832057953
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 82, Step 0Episode 82, Step 1Episode 82, Step 2Episode 82, Step 3Episode 82, Step 4Episode 82, Step 5Episode 82, Step 6Episode 82, Step 7Episode 82, Step 8Episode 82, Step 9

Episode 82 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Warning: Gradient norm overflow: 1.4145176978373647e+21, skipping update.
Episode 82 finished, Train info:
Value loss: 1.000000238418579
Action loss: 1.6880221664905548e-09
distEntropy loss: -0.22783012688159943
Loss: 0.4977218210697174
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 83, Step 0Episode 83, Step 1Episode 83, Step 2Episode 83, Step 3Episode 83, Step 4Episode 83, Step 5Episode 83, Step 6Episode 83, Step 7Episode 83, Step 8Episode 83, Step 9

Episode 83 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 83 finished, Train info:
Value loss: 1.0
Action loss: 4.6044586810012333e-08
distEntropy loss: -0.6888378858566284
Loss: 0.49311167001724243
total_episode_reward: 1.7881393432617188e-07
mean_episode_reward: 1.7881394143159923e-08
num_steps: 10
Episode 84, Step 0Episode 84, Step 1Episode 84, Step 2Episode 84, Step 3Episode 84, Step 4Episode 84, Step 5Episode 84, Step 6Episode 84, Step 7Episode 84, Step 8Episode 84, Step 9

Episode 84 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 84 finished, Train info:
Value loss: 1.000000238418579
Action loss: 3.5390257835388184e-08
distEntropy loss: -0.2596491277217865
Loss: 0.49740368127822876
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 85, Step 0Episode 85, Step 1Episode 85, Step 2Episode 85, Step 3Episode 85, Step 4Episode 85, Step 5Episode 85, Step 6Episode 85, Step 7Episode 85, Step 8Episode 85, Step 9

Episode 85 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 85 finished, Train info:
Value loss: 1.0
Action loss: 9.417534130307104e-08
distEntropy loss: -0.3116534352302551
Loss: 0.4968835711479187
total_episode_reward: 2.384185791015625e-07
mean_episode_reward: 2.3841858265427618e-08
num_steps: 10
Episode 86, Step 0Episode 86, Step 1Episode 86, Step 2Episode 86, Step 3Episode 86, Step 4Episode 86, Step 5Episode 86, Step 6Episode 86, Step 7Episode 86, Step 8Episode 86, Step 9

Episode 86 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 86 finished, Train info:
Value loss: 1.0
Action loss: 2.0265579436795633e-08
distEntropy loss: -0.29727703332901
Loss: 0.4970272183418274
total_episode_reward: -1.7881393432617188e-07
mean_episode_reward: -1.7881394143159923e-08
num_steps: 10
Episode 87, Step 0Episode 87, Step 1Episode 87, Step 2Episode 87, Step 3Episode 87, Step 4Episode 87, Step 5Episode 87, Step 6Episode 87, Step 7Episode 87, Step 8Episode 87, Step 9

Episode 87 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Warning: Gradient norm overflow: 1.3022930993958148e+21, skipping update.
Episode 87 finished, Train info:
Value loss: 1.0000001192092896
Action loss: -3.695488004495928e-08
distEntropy loss: -0.6931372880935669
Loss: 0.4930686354637146
total_episode_reward: -1.1920928955078125e-07
mean_episode_reward: -1.1920929132713809e-08
num_steps: 10
Episode 88, Step 0Episode 88, Step 1Episode 88, Step 2Episode 88, Step 3Episode 88, Step 4Episode 88, Step 5Episode 88, Step 6Episode 88, Step 7Episode 88, Step 8Episode 88, Step 9

Episode 88 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Warning: Gradient norm overflow: 1.0029330369562537e+21, skipping update.
Episode 88 finished, Train info:
Value loss: 1.0
Action loss: 1.3828277189986693e-07
distEntropy loss: -0.29907840490341187
Loss: 0.4970093369483948
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 89, Step 0Episode 89, Step 1Episode 89, Step 2Episode 89, Step 3Episode 89, Step 4Episode 89, Step 5Episode 89, Step 6Episode 89, Step 7Episode 89, Step 8Episode 89, Step 9

Episode 89 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 89 finished, Train info:
Value loss: 0.9999998211860657
Action loss: -3.6358834165639564e-08
distEntropy loss: -0.6762708425521851
Loss: 0.493237167596817
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 90, Step 0Episode 90, Step 1Episode 90, Step 2Episode 90, Step 3Episode 90, Step 4Episode 90, Step 5Episode 90, Step 6Episode 90, Step 7Episode 90, Step 8Episode 90, Step 9

Episode 90 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 90 finished, Train info:
Value loss: 1.0
Action loss: -1.3113021779531664e-08
distEntropy loss: -0.6661802530288696
Loss: 0.4933381974697113
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 91, Step 0Episode 91, Step 1Episode 91, Step 2Episode 91, Step 3Episode 91, Step 4Episode 91, Step 5Episode 91, Step 6Episode 91, Step 7Episode 91, Step 8Episode 91, Step 9

Episode 91 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 91 finished, Train info:
Value loss: 1.0000001192092896
Action loss: 1.7404556729161413e-07
distEntropy loss: -0.27581125497817993
Loss: 0.49724212288856506
total_episode_reward: -7.152557373046875e-07
mean_episode_reward: -7.152557657263969e-08
num_steps: 10
Episode 92, Step 0Episode 92, Step 1Episode 92, Step 2Episode 92, Step 3Episode 92, Step 4Episode 92, Step 5Episode 92, Step 6Episode 92, Step 7Episode 92, Step 8Episode 92, Step 9

Episode 92 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 92 finished, Train info:
Value loss: 0.9999998211860657
Action loss: 1.5161932154228452e-08
distEntropy loss: -0.3233940005302429
Loss: 0.49676600098609924
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 93, Step 0Episode 93, Step 1Episode 93, Step 2Episode 93, Step 3Episode 93, Step 4Episode 93, Step 5Episode 93, Step 6Episode 93, Step 7Episode 93, Step 8Episode 93, Step 9

Episode 93 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 93 finished, Train info:
Value loss: 0.9999998807907104
Action loss: -6.705522537231445e-08
distEntropy loss: -0.4412407875061035
Loss: 0.49558746814727783
total_episode_reward: -1.4901161193847656e-07
mean_episode_reward: -1.4901161193847656e-08
num_steps: 10
Episode 94, Step 0Episode 94, Step 1Episode 94, Step 2Episode 94, Step 3Episode 94, Step 4Episode 94, Step 5Episode 94, Step 6Episode 94, Step 7Episode 94, Step 8Episode 94, Step 9

Episode 94 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 4 has NaN/Inf, skipping update.
Episode 94 finished, Train info:
Value loss: 1.0
Action loss: -8.940696516468449e-10
distEntropy loss: -0.6662715673446655
Loss: 0.4933372735977173
total_episode_reward: -1.4901161193847656e-07
mean_episode_reward: -1.4901161193847656e-08
num_steps: 10
Episode 95, Step 0Episode 95, Step 1Episode 95, Step 2Episode 95, Step 3Episode 95, Step 4Episode 95, Step 5Episode 95, Step 6Episode 95, Step 7Episode 95, Step 8Episode 95, Step 9

Episode 95 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Warning: Gradient norm overflow: 1.6254083477004068e+21, skipping update.
Episode 95 finished, Train info:
Value loss: 1.0000001192092896
Action loss: 3.5762786065873797e-09
distEntropy loss: -0.6929823756217957
Loss: 0.49307024478912354
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 96, Step 0Episode 96, Step 1Episode 96, Step 2Episode 96, Step 3Episode 96, Step 4Episode 96, Step 5Episode 96, Step 6Episode 96, Step 7Episode 96, Step 8Episode 96, Step 9

Episode 96 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 96 finished, Train info:
Value loss: 0.9999998211860657
Action loss: 1.19209286886246e-09
distEntropy loss: -0.6931211352348328
Loss: 0.4930686950683594
total_episode_reward: -1.4901161193847656e-07
mean_episode_reward: -1.4901161193847656e-08
num_steps: 10
Episode 97, Step 0Episode 97, Step 1Episode 97, Step 2Episode 97, Step 3Episode 97, Step 4Episode 97, Step 5Episode 97, Step 6Episode 97, Step 7Episode 97, Step 8Episode 97, Step 9

Episode 97 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Episode 97 finished, Train info:
Value loss: 1.0
Action loss: 1.5702099176451156e-08
distEntropy loss: -0.3293148875236511
Loss: 0.49670684337615967
total_episode_reward: 0.0
mean_episode_reward: 0.0
num_steps: 10
Episode 98, Step 0Episode 98, Step 1Episode 98, Step 2Episode 98, Step 3Episode 98, Step 4Episode 98, Step 5Episode 98, Step 6Episode 98, Step 7Episode 98, Step 8Episode 98, Step 9

Episode 98 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 98 finished, Train info:
Value loss: 0.9999998807907104
Action loss: 1.8998980166884394e-08
distEntropy loss: -0.3317312002182007
Loss: 0.49668264389038086
total_episode_reward: -4.172325134277344e-07
mean_episode_reward: -4.17232506322307e-08
num_steps: 10
Episode 99, Step 0Episode 99, Step 1Episode 99, Step 2Episode 99, Step 3Episode 99, Step 4Episode 99, Step 5Episode 99, Step 6Episode 99, Step 7Episode 99, Step 8Episode 99, Step 9

Episode 99 begins to update:
traj finalize...
Optimizer state reset (momentum and iteration counter cleared).
Gradient 10 has NaN/Inf, skipping update.
Episode 99 finished, Train info:
Value loss: 0.9999998807907104
Action loss: -3.784894886393886e-08
distEntropy loss: -0.24556127190589905
Loss: 0.4975442886352539
total_episode_reward: -1.7881393432617188e-07
mean_episode_reward: -1.7881394143159923e-08
num_steps: 10
Training completed in 2.04263973236084 seconds.
ed layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
Actor and Critic backward pass completed.
Shared layer gradients computed.
